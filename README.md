# Codes for my M.Tech (Research) thesis

## Note
> The implementations are hard-coded. I will be uploading a clean version of my thesis codes soon. Thanks! And, meanwhile, if you have any question, please reach out to me here or on Twitter (@dbp_patel_1994).


## _BARE_
> Contains codes corresponding to the paper [_Adaptive Sample Selection for Robust Learning under Label Noise_](https://arxiv.org/abs/2106.15292) which corresponds to [Ch-3 of my master's thesis](https://dbp1994.github.io/files/deep-patel-iisc-masters-thesis_compressed.pdf). This work has been accepted at [WACV 2023](https://wacv2023.thecvf.com/home).

## memorization_and_overparam
> Contains codes corresponding to the paper [_Memorization in Deep Neural Networks: Does the Loss Function Matter?_](https://link.springer.com/chapter/10.1007/978-3-030-75765-6_11) which corresponds to [Ch-4 of my master's thesis](https://dbp1994.github.io/files/deep-patel-iisc-masters-thesis_compressed.pdf).


## Things to be done:
- [X] Create a file for the conda environment details [**_conda_env_details.txt_**)]
- [ ] Create a file containing hyperparameter details for all the experiments [**_hyperparam_details.md_**] [[desired format](https://github.com/HanxunH/Active-Passive-Losses/blob/master/configs/cifar10/sym/gce.yaml)]
- [ ] Upload all the baseline codes in **_BARE_** folder
- [X] Upload codes for my algorithms
- [X] Upload codes pertaining to experiments on memorization and overparameterization
- [] _Modularize_ your codes (along the lines of [this](https://github.com/hrayrhar/limit-label-memorization/releases/tag/v0.1)/[this](https://github.com/hrayrhar/limit-label-memorization) PyTorch-based repository)

## Instructions to run experiments:
- To be upated.

